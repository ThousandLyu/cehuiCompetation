# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'SystemUI.ui'
#
# Created by: PyQt5 UI code generator 5.15.6
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.
import os
import socket
import threading
import time

import cv2
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import QUrl, QThread, pyqtSignal
from PyQt5.QtWebEngineWidgets import QWebEngineView
from PyQt5.QtWidgets import QVBoxLayout, QGridLayout

import torch.nn as nn
import argparse
import sys
from pathlib import Path
import numpy as np
import torch
import torch.backends.cudnn as cudnn

nn.Module.dump_patches = True

img_yolo = None
rtmp_address = 0
stop_runnning = False
yolo_client=None

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.experimental import attempt_load
from utils.datasets import LoadImages, LoadStreams
from utils.general import apply_classifier, check_img_size, check_imshow, check_requirements, check_suffix, colorstr, \
    increment_path, non_max_suppression, print_args, save_one_box, scale_coords, set_logging, \
    strip_optimizer, xyxy2xywh
from utils.plots import Annotator, colors
from utils.torch_utils import load_classifier, select_device, time_sync


@torch.no_grad()
def run(weights=ROOT / 'yolov5s.pt',  # 权重参数路径
        source=ROOT / 'data/images',  # 资源路径
        imgsz=640,  # 输入图像大小
        conf_thres=0.25,  # 置信度阈值
        iou_thres=0.45,  # IOU阈值
        max_det=1000,  # 每张图片最大预测数量
        device='',  # 与测试用的设备(cpu,gpu)
        view_img=False,  # 显示结果
        save_txt=False,  # 保存到文本
        save_conf=False,  # 保存置信度
        save_crop=False,  # 保存识别框
        nosave=False,  # 不保存视频和图片
        classes=None,  # 类别过滤
        agnostic_nms=False,  # 前景背景分类
        augment=False,  # 增广推理
        visualize=False, #可视化
        update=False,  # 更新所有模型
        project=ROOT / 'runs/detect',  # 视频图片保存路径
        name='exp',  # 保存文件夹名称
        exist_ok=False,  # 文件已存在,则不增加数量
        line_thickness=1,  # 识别框粗细
        hide_labels=True,  # 隐藏标签
        hide_conf=False,  # 隐藏置信度
        half=False,  # FP16 半浮点数
        dnn=False,  # 动态网络
        ):
    global img_yolo,stop_runnning,yolo_client



    source = str(source)
    save_img = not nosave and not source.endswith('.txt')  # save inference images
    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(
        ('rtsp://', 'rtmp://', 'http://', 'https://'))

    # Directories
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Initialize
    set_logging()
    device = select_device(device)
    half &= device.type != 'cpu'  # half precision only supported on CUDA

    # Load model
    w = str(weights[0] if isinstance(weights, list) else weights)
    classify, suffix, suffixes = False, Path(w).suffix.lower(), ['.pt', '.onnx', '.tflite', '.pb', '']
    check_suffix(w, suffixes)  # check weights have acceptable suffix
    pt, onnx, tflite, pb, saved_model = (suffix == x for x in suffixes)  # backend booleans
    stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults
    if pt:
        model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)
        stride = int(model.stride.max())  # model stride
        names = model.module.names if hasattr(model, 'module') else model.names  # get class names
        if half:
            model.half()  # to FP16
        if classify:  # second-stage classifier
            modelc = load_classifier(name='resnet50', n=2)  # initialize
            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()
    elif onnx:
        if dnn:
            # check_requirements(('opencv-python>=4.5.4',))
            net = cv2.dnn.readNetFromONNX(w)
        else:
            check_requirements(('onnx', 'onnxruntime'))
            import onnxruntime
            session = onnxruntime.InferenceSession(w, None)
    else:  # TensorFlow models
        check_requirements(('tensorflow>=2.4.1',))
        import tensorflow as tf
        if pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
            def wrap_frozen_graph(gd, inputs, outputs):
                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped import
                return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),
                               tf.nest.map_structure(x.graph.as_graph_element, outputs))

            graph_def = tf.Graph().as_graph_def()
            graph_def.ParseFromString(open(w, 'rb').read())
            frozen_func = wrap_frozen_graph(gd=graph_def, inputs="x:0", outputs="Identity:0")
        elif saved_model:
            model = tf.keras.models.load_model(w)
        elif tflite:
            interpreter = tf.lite.Interpreter(model_path=w)  # load TFLite model
            interpreter.allocate_tensors()  # allocate
            input_details = interpreter.get_input_details()  # inputs
            output_details = interpreter.get_output_details()  # outputs
            int8 = input_details[0]['dtype'] == np.uint8  # is TFLite quantized uint8 model
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    if webcam:
        view_img = check_imshow()
        cudnn.benchmark = True  # set True to speed up constant image size inference
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)
        bs = len(dataset)  # batch_size
    else:
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)
        bs = 1  # batch_size
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    if pt and device.type != 'cpu':
        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once
    dt, seen = [0.0, 0.0, 0.0], 0
    for path, img, im0s, vid_cap in dataset:
        if stop_runnning == True:
            break
        t1 = time_sync()
        if onnx:
            img = img.astype('float32')
        else:
            img = torch.from_numpy(img).to(device)
            img = img.half() if half else img.float()  # uint8 to fp16/32
        img = img / 255.0  # 0 - 255 to 0.0 - 1.0
        if len(img.shape) == 3:
            img = img[None]  # expand for batch dim
        t2 = time_sync()
        dt[0] += t2 - t1

        # Inference
        if pt:
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            pred = model(img, augment=augment, visualize=visualize)[0]
        elif onnx:
            if dnn:
                net.setInput(img)
                pred = torch.tensor(net.forward())
            else:
                pred = torch.tensor(session.run([session.get_outputs()[0].name], {session.get_inputs()[0].name: img}))
        else:  # tensorflow model (tflite, pb, saved_model)
            imn = img.permute(0, 2, 3, 1).cpu().numpy()  # image in numpy
            if pb:
                pred = frozen_func(x=tf.constant(imn)).numpy()
            elif saved_model:
                pred = model(imn, training=False).numpy()
            elif tflite:
                if int8:
                    scale, zero_point = input_details[0]['quantization']
                    imn = (imn / scale + zero_point).astype(np.uint8)  # de-scale
                interpreter.set_tensor(input_details[0]['index'], imn)
                interpreter.invoke()
                pred = interpreter.get_tensor(output_details[0]['index'])
                if int8:
                    scale, zero_point = output_details[0]['quantization']
                    pred = (pred.astype(np.float32) - zero_point) * scale  # re-scale
            pred[..., 0] *= imgsz[1]  # x
            pred[..., 1] *= imgsz[0]  # y
            pred[..., 2] *= imgsz[1]  # w
            pred[..., 3] *= imgsz[0]  # h
            pred = torch.tensor(pred)
        t3 = time_sync()
        dt[1] += t3 - t2

        # NMS
        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
        dt[2] += time_sync() - t3

        # Second-stage classifier (optional)
        if classify:
            pred = apply_classifier(pred, modelc, img, im0s)

        # Process predictions
        for i, det in enumerate(pred):  # per image
            if stop_runnning:
                pred = None
                break
            seen += 1
            if webcam:  # batch_size >= 1
                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count
            else:
                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)

            p = Path(p)  # to Path
            save_path = str(save_dir / p.name)  # img.jpg
            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt
            s += '%gx%g ' % img.shape[2:]  # print string
            objectclass = ""
            count = [0, 0]
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            imc = im0.copy() if save_crop else im0  # for save_crop
            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                for c in det[:, -1].unique():
                    n = (det[:, -1] == c).sum()  # detections per class
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string
                    count[int(c)] = int(n)

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    if save_txt:  # Write to file
                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                        with open(txt_path + '.txt', 'a') as f:
                            f.write(('%g ' * len(line)).rstrip() % line + '\n')

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')
                        annotator.box_label(xyxy, label, color=colors(c, True))
                        if save_crop:
                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)

            person_num = count[0]

            yolo_client.send((str(person_num)).encode(encoding='utf-8'))  # 发送数据

            # Stream results
            im0 = annotator.result()
            if view_img:
                objectclass = "person " + str(count[0])
                im0 = cv2.putText(img=im0, text=objectclass, org=(20, 40), fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                                  fontScale=1, color=(255, 255, 255), thickness=2)
                # cv2.imshow(str(p), im0)
                img_yolo = im0
                cv2.waitKey(1)  # 1 millisecond

            # Save results (image with detections)
            if save_img:
                if dataset.mode == 'image':
                    cv2.imwrite(save_path, im0)
                else:  # 'video' or 'stream'
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()  # release previous video writer
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                            save_path += '.mp4'
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                    vid_writer[i].write(im0)

    # Print results
    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image
    print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    if save_txt or save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        print(f"Results saved to {colorstr('bold', save_dir)}{s}")
    if update:
        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)


def parse_opt():
    global rtmp_address
    parser = argparse.ArgumentParser()

    parser.add_argument('--weights', nargs='+', type=str,
                        default=r'./weights/CNY20.pt', help='model path(s)')
    parser.add_argument('--source', type=str,
                        default=rtmp_address, help='file/dir/URL/glob, 0 for webcam')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results', default=True)
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=2, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=True, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=True, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(FILE.stem, opt)
    return opt


def detect_person(opt):
    # 启动yolo服务器
    check_requirements(exclude=('tensorboard', 'thop'))
    print("YOLO人脸识别程序 启动")
    run(**vars(opt))


def start_yolo_service():

    opt = parse_opt()
    detect_person(opt)


# start_yolo_service()


def execCmd(cmd):
    try:
        print("命令 %s 开始运行" % (cmd))
        os.system(cmd)
        print("命令 %s 结束运行" % (cmd))
    except:
        print('%s\t 运行失败' % (cmd))


terminalText = ""


class Transport_Eyes(object):

    def setupUi(self, Dialog):
        self.threads = []

        Dialog.setObjectName("Dialog")
        Dialog.resize(1252, 804)
        self.lb_IP_Address = QtWidgets.QLabel(Dialog)
        self.lb_IP_Address.setGeometry(QtCore.QRect(20, 30, 71, 31))
        font = QtGui.QFont()
        font.setFamily("AcadEref")
        font.setPointSize(12)
        self.lb_IP_Address.setFont(font)
        self.lb_IP_Address.setObjectName("lb_IP_Address")
        self.le_IP_Address = QtWidgets.QLineEdit(Dialog)
        self.le_IP_Address.setGeometry(QtCore.QRect(90, 30, 231, 31))
        self.le_IP_Address.setObjectName("le_IP_Address")
        self.btn_Get_IP = QtWidgets.QPushButton(Dialog)
        self.btn_Get_IP.setGeometry(QtCore.QRect(330, 30, 91, 31))
        self.btn_Get_IP.setObjectName("btn_Get_IP")
        self.frame = QtWidgets.QFrame(Dialog)
        self.frame.setGeometry(QtCore.QRect(10, 70, 1231, 191))
        self.frame.setFrameShape(QtWidgets.QFrame.Panel)
        self.frame.setFrameShadow(QtWidgets.QFrame.Raised)
        self.frame.setObjectName("frame")
        self.btn_Start_RTMP = QtWidgets.QPushButton(self.frame)
        self.btn_Start_RTMP.setGeometry(QtCore.QRect(360, 10, 91, 31))
        self.btn_Start_RTMP.setObjectName("btn_Start_RTMP")
        self.lb_RTMP_Service = QtWidgets.QLabel(self.frame)
        self.lb_RTMP_Service.setGeometry(QtCore.QRect(20, 10, 141, 31))
        font = QtGui.QFont()
        font.setFamily("AcadEref")
        font.setPointSize(12)
        self.lb_RTMP_Service.setFont(font)
        self.lb_RTMP_Service.setObjectName("lb_RTMP_Service")
        self.le_RTMP_Service = QtWidgets.QLineEdit(self.frame)
        self.le_RTMP_Service.setEnabled(True)
        self.le_RTMP_Service.setGeometry(QtCore.QRect(170, 10, 181, 31))
        self.le_RTMP_Service.setReadOnly(True)
        self.le_RTMP_Service.setObjectName("le_RTMP_Service")
        self.frame_2 = QtWidgets.QFrame(Dialog)
        self.frame_2.setGeometry(QtCore.QRect(10, 270, 351, 521))
        self.frame_2.setFrameShape(QtWidgets.QFrame.Panel)
        self.frame_2.setFrameShadow(QtWidgets.QFrame.Raised)
        self.frame_2.setObjectName("frame_2")
        self.btn_Start_TCP = QtWidgets.QPushButton(self.frame_2)
        self.btn_Start_TCP.setGeometry(QtCore.QRect(250, 30, 91, 31))
        self.btn_Start_TCP.setObjectName("btn_Start_TCP")
        self.le_TCP_Service = QtWidgets.QLineEdit(self.frame_2)
        self.le_TCP_Service.setEnabled(True)
        self.le_TCP_Service.setGeometry(QtCore.QRect(170, 30, 71, 31))
        self.le_TCP_Service.setReadOnly(True)
        self.le_TCP_Service.setObjectName("le_TCP_Service")
        self.te_Terminal_2 = QtWidgets.QTextEdit(self.frame_2)
        self.te_Terminal_2.setGeometry(QtCore.QRect(10, 130, 321, 351))
        self.te_Terminal_2.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.te_Terminal_2.setObjectName("te_Terminal_2")
        self.lb_TCP_Service = QtWidgets.QLabel(self.frame_2)
        self.lb_TCP_Service.setGeometry(QtCore.QRect(20, 30, 141, 31))
        font = QtGui.QFont()
        font.setFamily("AcadEref")
        font.setPointSize(12)
        self.lb_TCP_Service.setFont(font)
        self.lb_TCP_Service.setTextFormat(QtCore.Qt.AutoText)
        self.lb_TCP_Service.setObjectName("lb_TCP_Service")
        self.lb_TCP_Service_3 = QtWidgets.QLabel(self.frame_2)
        self.lb_TCP_Service_3.setGeometry(QtCore.QRect(20, 90, 241, 31))
        font = QtGui.QFont()
        font.setFamily("AcadEref")
        font.setPointSize(12)
        self.lb_TCP_Service_3.setFont(font)
        self.lb_TCP_Service_3.setTextFormat(QtCore.Qt.AutoText)
        self.lb_TCP_Service_3.setObjectName("lb_TCP_Service_3")
        self.frame_4 = QtWidgets.QFrame(Dialog)
        self.frame_4.setGeometry(QtCore.QRect(370, 270, 871, 521))
        self.frame_4.setFrameShape(QtWidgets.QFrame.Panel)
        self.frame_4.setFrameShadow(QtWidgets.QFrame.Raised)
        self.frame_4.setObjectName("frame_4")
        self.label = QtWidgets.QLabel(self.frame_4)
        self.label.setGeometry(QtCore.QRect(20, 50, 841, 461))
        self.label.setFrameShape(QtWidgets.QFrame.Box)
        self.label.setObjectName("label")
        self.lb_YOLO = QtWidgets.QLabel(self.frame_4)
        self.lb_YOLO.setGeometry(QtCore.QRect(40, 10, 121, 31))
        font = QtGui.QFont()
        font.setFamily("AcadEref")
        font.setPointSize(12)
        self.lb_YOLO.setFont(font)
        self.lb_YOLO.setTextFormat(QtCore.Qt.AutoText)
        self.lb_YOLO.setObjectName("lb_YOLO")
        self.btn_Start_YOLO = QtWidgets.QPushButton(self.frame_4)
        self.btn_Start_YOLO.setGeometry(QtCore.QRect(530, 10, 91, 31))
        self.btn_Start_YOLO.setObjectName("btn_Start_YOLO")
        self.le_YOLO = QtWidgets.QLineEdit(self.frame_4)
        self.le_YOLO.setEnabled(True)
        self.le_YOLO.setGeometry(QtCore.QRect(170, 10, 341, 31))
        self.le_YOLO.setReadOnly(False)
        self.le_YOLO.setObjectName("le_YOLO")
        self.frame_4.raise_()
        self.frame_2.raise_()
        self.frame.raise_()
        self.lb_IP_Address.raise_()
        self.le_IP_Address.raise_()
        self.btn_Get_IP.raise_()



        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)

        # 绑定点击事件
        # 显示IP地址
        self.btn_Get_IP.clicked.connect(self.show_IP)
        # 连接RTMP服务器
        self.btn_Start_RTMP.clicked.connect(self.Start_RTMP)
        # 连接信息转发服务器
        self.btn_Start_TCP.clicked.connect(self.Start_TCP)
        # 连接人脸识别程序
        self.btn_Start_YOLO.clicked.connect(self.Start_YOLO)

        self.click_TCP = False
        self.click_yolo = False
        self.click_RTMP = False


    def Start_YOLO(self):

        global stop_runnning,yolo_client

        if self.threads != None:
            stop_runnning = True
            self.threads.clear()

        if yolo_client == None:
            # TODO 创建客户端
            yolo_client = socket.socket()
            yolo_client.connect((self.ip, 8080))  # 连接服务器
            yolo_client.send("YOLO".encode(encoding='utf-8'))  # 发送数据

        global rtmp_address

        rtmp_address=self.le_YOLO.text()

        time.sleep(1)
        stop_runnning = False


        th = threading.Thread(target=start_yolo_service)
        th.start()
        self.threads.append(th)

        self.fresh_img = Timer01()
        self.fresh_img._signal.connect(self.show_yolo_video)
        self.fresh_img.start()



    def Start_TCP(self):
        if self.click_TCP == False:
            cmd = 'python TCP_service.py'
            th = threading.Thread(target=execCmd, args=(cmd,))
            th.start()
            self.le_TCP_Service.setText("已启动")

            self.fresh_terminal = Thread_get_data(self.ip)
            self.fresh_terminal._signal.connect(self.show_terminal)
            self.click_TCP = True

        self.fresh_terminal.start()

    def show_terminal(self, msg):
        global terminalText
        terminalText += msg + '\n'
        self.te_Terminal_2.setText(terminalText)
        self.Start_TCP()



    def show_yolo_video(self):

        # global img_yolo
        try:
            shrink = cv2.cvtColor(img_yolo, cv2.COLOR_BGR2RGB)
            QtImg = QtGui.QImage(shrink.data,
                                 shrink.shape[1],
                                 shrink.shape[0],
                                 shrink.shape[1] * 3,
                                 QtGui.QImage.Format_RGB888)

            jpg_out = QtGui.QPixmap(QtImg).scaled(
                self.label.width(), self.label.height())

            self.label.setPixmap(jpg_out)
        except:
            print("加载失败")
            time.sleep(1)



    def Start_RTMP(self):
        if self.click_RTMP == False:
            cmd = 'cd rtmp/nginx_1.7.11.3/ && nginx.exe -c conf/nginx.conf'
            th = threading.Thread(target=execCmd, args=(cmd,))
            th.start()
            self.le_RTMP_Service.setText("已连接")

            self.webview = QWebEngineView()
            vbox = QVBoxLayout()
            vbox.addWidget(self.webview)
            main = QGridLayout()
            main.addLayout(vbox, 0, 0)
            self.frame.setLayout(main)

            self.reload_web_Timer = Timer02()
            self.reload_web_Timer._signal.connect(self.reload_web)
            self.reload_web_Timer.start()

            self.webview.load(QUrl("http://localhost/stat"))
            self.webview.show()
            self.click_RTMP=True

        self.webview.reload()

    def reload_web(self):
        self.webview.reload()


    def show_IP(self):
        # 获取ip地址
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        self.ip = str(s.getsockname()[0])
        self.le_IP_Address.setText(self.ip)

        self.le_YOLO.setText("rtmp://" + self.ip + ":1935" + "/live/stream1")

    def retranslateUi(self, Dialog):
        _translate = QtCore.QCoreApplication.translate
        Dialog.setWindowTitle(_translate("Dialog", "Dialog"))
        self.lb_IP_Address.setText(_translate("Dialog", "IP地址"))
        self.btn_Get_IP.setText(_translate("Dialog", "获取IP地址"))
        self.btn_Start_RTMP.setText(_translate("Dialog", "启动"))
        self.lb_RTMP_Service.setText(_translate("Dialog", "视频传输服务器"))
        self.le_RTMP_Service.setText(_translate("Dialog", "未启动"))
        self.btn_Start_TCP.setText(_translate("Dialog", "启动"))
        self.le_TCP_Service.setText(_translate("Dialog", "未启动"))
        self.lb_TCP_Service.setText(_translate("Dialog", "信息转发服务器"))
        self.lb_TCP_Service_3.setText(_translate("Dialog", "信息转发服务器终端"))
        self.label.setText(_translate("Dialog", "识别结果"))
        self.lb_YOLO.setText(_translate("Dialog", "人脸识别程序"))
        self.btn_Start_YOLO.setText(_translate("Dialog", "启动"))

# 接收数据
class Thread_get_data(QThread):
    _signal = pyqtSignal(str)

    def __init__(self, ip):
        super().__init__()
        self.ip = ip

    def run(self):
        # 创建客户端
        time.sleep(1)
        client = socket.socket()
        client.connect((self.ip, 8080))  # 连接服务器
        client.send("main".encode(encoding='utf-8'))  # 发送数据
        while True:
            recv_data = str(client.recv(1024), encoding="utf-8")
            self._signal.emit(recv_data);

# 刷新yolo界面
class Timer01(QThread):

    _signal = pyqtSignal()

    def __init__(self):
        super().__init__()
        global img_yolo

    def run(self):
        while True:
            self._signal.emit()
            time.sleep(0.05)

# 刷新rtmp服务器界面
class Timer02(QThread):

    _signal = pyqtSignal()

    def __init__(self):
        super().__init__()

    def run(self):
        while True:
            self._signal.emit()
            time.sleep(1)




if __name__ == '__main__':
    import sys

    app = QtWidgets.QApplication(sys.argv)
    app.setStyle('Fusion')  # 设置窗口风格 “Windows”，“WindowsXP”，“WindowsVista”，“Fusion”四种风格
    MainWindow = QtWidgets.QMainWindow()  # 创建窗体对象
    ui = Transport_Eyes()  # 创建pyqt设计的窗体对象
    ui.setupUi(MainWindow)  # 调用pyqt窗体的方法对窗体对象进行初始化设置
    MainWindow.show()  # 显示窗体
    sys.exit(app.exec_())  # 程序关闭时退出进程
